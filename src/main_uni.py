# main_uni.py - VERSÃƒO PRODUÃ‡ÃƒO COM TREINAMENTO DESATIVADO

import warnings
import argparse
import sys
import os
import logging
import time
from pathlib import Path
from typing import Optional
import json

# âœ… Silencia TUDO
os.environ["ORT_LOGGING_LEVEL"] = "3"
os.environ["ONNX_LOGGING_LEVEL"] = "3"
warnings.filterwarnings("ignore")

def force_delete_onnx(weights_path: str) -> None:
    """âœ… APAGA ONNX SEMPRE"""
    onnx_path = Path(weights_path).with_suffix(".onnx")
    if onnx_path.exists():
        print(f"ğŸ—‘ï¸ Apagando ONNX existente: {onnx_path.name}")
        onnx_path.unlink()

def find_config_file():
    """Encontra arquivo de configuraÃ§Ã£o disponÃ­vel"""
    possible_configs = [
        "config.json",
        "src/config.json", 
        Path.cwd() / "config.json",
        Path.cwd() / "src" / "config.json"
    ]
    
    for config_path in possible_configs:
        if Path(config_path).exists():
            print(f"âœ… Usando config: {config_path}")
            return str(config_path)
    
    print("âš ï¸ Config nÃ£o encontrado, criando config padrÃ£o...")
    return create_default_config()

def create_default_config(config_path="config.json"):
    """Cria configuraÃ§Ã£o padrÃ£o baseada no hardware"""
    
    # Detectar hardware disponÃ­vel
    try:
        import torch
        has_cuda = torch.cuda.is_available()
        if has_cuda:
            gpu_name = torch.cuda.get_device_name()
            print(f"ğŸ® GPU detectada: {gpu_name}")
        else:
            print("ğŸ’» Usando CPU")
    except ImportError:
        has_cuda = False
        print("ğŸ’» PyTorch nÃ£o encontrado, assumindo CPU")
    
    try:
        import psutil
        cpu_cores = psutil.cpu_count(logical=False)
        optimal_threads = min(cpu_cores, 12)
    except ImportError:
        optimal_threads = 8
    
    # âœ… USAR SEU CONFIG COMO BASE - FOCO EM PRODUÃ‡ÃƒO
    config = {
        "tracking": {
            "tracker": "bytetrack.yaml",
            "inference": {
                "device": "cpu",
                "half_precision": False
            },
            "detection": {
                "imgsz_cpu": 320,
                "conf_threshold": 0.45,
                "iou_threshold": 0.6,
                "max_det": 25
            },
            "classification_model": {
                "imgsz": 128,
                "batch_size": 10,
                "center_crop_margin": 0.15,
                "min_crop_size": 10
            },
            "classification": {
                "min_confidence": 0.35
            },
            "sampling": {
                "classify_every": 5
            },
            "performance": {
                "frame_stride": 2,
                "detection_interval": 3,
                "num_threads_cpu": optimal_threads,
                "use_onnx": False,
                "force_pytorch": True,
                "force_classification_pytorch": True,
                "enable_warmup": True,
                "warmup_iterations": 3
            },
            "visualization": {
                "draw_boxes": True,
                "draw_track_id": True,
                "draw_labels": True,
                "draw_confidence": True,
                "box_thickness": 2,
                "font_scale": 0.6,
                "font_thickness": 2
            },
            "output": {
                "save_video": True,
                "save_json": True,
                "save_csv": True,
                "video_codec": "mp4v",
                "timeline_mode": "duplicate"
            }
        },
        "api": {
            "server": {
                "host": "127.0.0.1",
                "port": 8000,
                "workers": 1,
                "reload": False,
                "debug": False
            },
            "upload": {
                "max_file_size": 500,
                "allowed_extensions": [".mp4", ".avi", ".mov", ".mkv"],
                "upload_dir": "uploads",
                "cleanup_after_hours": 24
            },
            "processing": {
                "queue_size": 10,
                "concurrent_jobs": 2,
                "timeout_minutes": 30
            },
            "security": {
                "rate_limit": 10,
                "rate_window_minutes": 1
            }
        },
        "system": {
            "production_mode": True,
            "training_disabled": True,
            "cicd_pipeline": "future_implementation"
        }
    }
    
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"âœ… Config de produÃ§Ã£o criado: {config_path}")
    return config_path

def validate_config(config_path):
    """Valida se o config tem as seÃ§Ãµes necessÃ¡rias"""
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        required_sections = ["tracking"]
        missing = [section for section in required_sections if section not in config]
        
        if missing:
            print(f"âš ï¸ SeÃ§Ãµes ausentes no config: {missing}")
            return False
        
        print(f"âœ… Config vÃ¡lido: {config_path}")
        return True
        
    except json.JSONDecodeError as e:
        print(f"âŒ Config JSON invÃ¡lido: {e}")
        return False
    except Exception as e:
        print(f"âŒ Erro lendo config: {e}")
        return False

def cmd_track_direct(video_path, det_weights, cls_weights, output_dir=""):
    """Tracking direto otimizado - PRODUÃ‡ÃƒO"""
    try:
        config_path = find_config_file()
        
        if not validate_config(config_path):
            print("âŒ Config invÃ¡lido. Abortando...")
            return None
        
        # Limpeza ONNX
        print("ğŸ”„ Limpando arquivos ONNX...")
        force_delete_onnx(det_weights)
        force_delete_onnx(cls_weights)
        
        # ValidaÃ§Ã£o de arquivos
        validations = [
            (video_path, "VÃ­deo", "ğŸ“¹"),
            (det_weights, "Detector", "ğŸ¤–"),
            (cls_weights, "Classificador", "ğŸ¨")
        ]
        
        for file_path, name, icon in validations:
            if not Path(file_path).exists():
                print(f"âŒ {name} nÃ£o encontrado: {file_path}")
                return None
            else:
                size_mb = Path(file_path).stat().st_size / (1024**2)
                print(f"âœ… {icon} {name}: {file_path} ({size_mb:.1f}MB)")
        
        print(f"âš™ï¸ Config: {config_path}")
        
        # DiagnÃ³stico
        print("\nğŸ” === DIAGNÃ“STICO PRE-PROCESSAMENTO ===")
        
        try:
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name()
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                print(f"ğŸ® GPU: {gpu_name} ({gpu_memory:.1f}GB)")
            else:
                print("ğŸ’» Dispositivo: CPU")
                
            print(f"ğŸ”¥ PyTorch: {torch.__version__}")
        except ImportError:
            print("âš ï¸ PyTorch nÃ£o encontrado")
        
        try:
            import psutil
            cpu_count = psutil.cpu_count()
            cpu_freq = psutil.cpu_freq()
            ram_total = psutil.virtual_memory().total / (1024**3)
            print(f"ğŸ’» CPU: {cpu_count} cores @ {cpu_freq.current:.0f}MHz")
            print(f"ğŸ’¾ RAM: {ram_total:.1f}GB")
        except ImportError:
            print("âš ï¸ psutil nÃ£o encontrado")
        
        try:
            from ultralytics import YOLO
            print("ğŸ¤– Ultralytics: âœ… DisponÃ­vel")
        except ImportError:
            print("âŒ Ultralytics nÃ£o encontrado! pip install ultralytics")
            return None
        
        # Executa tracking
        print("\nğŸš€ Iniciando tracking otimizado...")
        from src.tracking.track import process_video_tracking
        
        start_total = time.time()
        
        result = process_video_tracking(
            video_path=video_path,
            det_weights=det_weights,
            cls_weights=cls_weights,
            config_path=config_path,
            out_dir=output_dir if output_dir else None,
        )
        
        total_time = time.time() - start_total
        
        if result:
            print(f"\nğŸ‰ === TRACKING CONCLUÃDO ===")
            print(f"â±ï¸ Tempo total: {total_time:.1f}s")
            print(f"ğŸ“Š Tracks detectados: {result.get('total_tracks', 0)}")
            
            # Performance metrics
            if 'performance' in result:
                perf = result['performance']
                print(f"âš¡ FPS mÃ©dio: {perf.get('average_fps', 0):.1f}")
                print(f"ğŸš€ FPS equivalente: {perf.get('fps_raw_equiv', 0):.1f}")
            
            # DistribuiÃ§Ã£o de cores
            if 'color_distribution' in result and result['color_distribution']:
                print("ğŸ¨ DistribuiÃ§Ã£o de cores:")
                total_vehicles = sum(result['color_distribution'].values())
                for color, count in result['color_distribution'].items():
                    percentage = (count / total_vehicles * 100) if total_vehicles > 0 else 0
                    print(f"   {color}: {count} veÃ­culos ({percentage:.1f}%)")
            
            # Arquivos gerados
            if 'output_files' in result and result['output_files']:
                print("ğŸ“ Arquivos gerados:")
                for file_type, file_path in result['output_files'].items():
                    if file_path and Path(file_path).exists():
                        size_mb = Path(file_path).stat().st_size / (1024**2)
                        print(f"   {file_type}: {Path(file_path).name} ({size_mb:.1f}MB)")
        else:
            print("\nâŒ Processamento falhou.")
        
        return result
        
    except Exception as e:
        print(f"âŒ Erro no tracking: {e}")
        import traceback
        traceback.print_exc()
        return None

def cmd_api_integrated(host="127.0.0.1", port=8000, reload=False):
    """âœ… API INTEGRADA COM MAIN_API.PY COMPLETA"""
    try:
        print("ğŸš€ === INICIANDO API INTEGRADA ===")
        
        # âœ… VERIFICAR SE MAIN_API.PY EXISTE
        api_paths = [
            "src/api/main_api.py",
            "main_api.py",
            "src/main_api.py"
        ]
        
        api_file = None
        for api_path in api_paths:
            if Path(api_path).exists():
                api_file = api_path
                print(f"âœ… API encontrada: {api_path}")
                break
        
        if not api_file:
            print("âš ï¸ main_api.py nÃ£o encontrado, criando API bÃ¡sica...")
            return cmd_api_basic(host, port)
        
        # âœ… TENTAR IMPORTAR E EXECUTAR API COMPLETA
        try:
            import uvicorn
            print("âœ… Uvicorn disponÃ­vel")
            
            # Verificar dependÃªncias da API
            try:
                import fastapi
                from fastapi import FastAPI, UploadFile, File, HTTPException
                from fastapi.responses import JSONResponse, FileResponse
                print("âœ… FastAPI disponÃ­vel")
            except ImportError as e:
                print(f"âŒ FastAPI nÃ£o disponÃ­vel: {e}")
                print("ğŸ’¡ Instale: pip install fastapi uvicorn python-multipart")
                return None
            
            # âœ… IMPORTAR E EXECUTAR API COMPLETA
            if api_file == "src/api/main_api.py":
                from src.api.main_api import app
                print("âœ… API completa carregada de src/api/main_api.py")
            elif api_file == "main_api.py":
                from main_api import app
                print("âœ… API completa carregada de main_api.py")
            elif api_file == "src/main_api.py":
                from src.main_api import app
                print("âœ… API completa carregada de src/main_api.py")
            
            print(f"ğŸŒ Servidor: http://{host}:{port}")
            print(f"ğŸ“– DocumentaÃ§Ã£o: http://{host}:{port}/docs")
            print(f"ğŸ”§ Swagger UI: http://{host}:{port}/redoc")
            print("ğŸ’¡ Pressione Ctrl+C para parar")
            
            if reload:
                print("ğŸ”„ Modo desenvolvimento (auto-reload)")
                if api_file == "src/api/main_api.py":
                    uvicorn.run("src.api.main_api:app", host=host, port=port, reload=True, workers=1)
                elif api_file == "main_api.py":
                    uvicorn.run("main_api:app", host=host, port=port, reload=True, workers=1)
                elif api_file == "src/main_api.py":
                    uvicorn.run("src.main_api:app", host=host, port=port, reload=True, workers=1)
            else:
                uvicorn.run(app, host=host, port=port, reload=False, workers=1)
            
            return {"success": True}
            
        except ImportError as e:
            print(f"âŒ Erro importando API: {e}")
            print("ğŸ“‹ Falling back to basic API...")
            return cmd_api_basic(host, port)
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ API interrompida pelo usuÃ¡rio")
        return {"success": True, "message": "Interrompido"}
    except Exception as e:
        print(f"âŒ Erro na API integrada: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def cmd_api_basic(host="127.0.0.1", port=8000):
    """API bÃ¡sica como fallback"""
    try:
        import uvicorn
        from fastapi import FastAPI, UploadFile, File, HTTPException
        from fastapi.responses import JSONResponse
        from fastapi.middleware.cors import CORSMiddleware
        
        print("ğŸ”§ Criando API bÃ¡sica (fallback)...")
        
        app = FastAPI(
            title="Vox Color Detection API - Production",
            description="API de produÃ§Ã£o para detecÃ§Ã£o e classificaÃ§Ã£o de cores de veÃ­culos",
            version="2.0.0"
        )
        
        # CORS
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        @app.get("/")
        def root():
            return {
                "message": "Vox API - Production Mode",
                "status": "ok",
                "version": "2.0.0",
                "mode": "production",
                "training": "disabled"
            }
        
        @app.get("/health")
        def health():
            try:
                import torch
                cuda_available = torch.cuda.is_available()
                if cuda_available:
                    gpu_name = torch.cuda.get_device_name()
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    gpu_info = f"{gpu_name} ({gpu_memory:.1f}GB)"
                else:
                    gpu_info = "N/A"
            except:
                cuda_available = False
                gpu_info = "N/A"
            
            return {
                "status": "healthy",
                "mode": "production",
                "cuda_available": cuda_available,
                "gpu_info": gpu_info,
                "config_exists": Path("config.json").exists(),
                "tracking_available": Path("src/tracking/track.py").exists(),
                "training_disabled": True
            }
        
        @app.post("/upload")
        async def upload_video(file: UploadFile = File(...)):
            if not file.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):
                raise HTTPException(status_code=400, detail="Formato de vÃ­deo nÃ£o suportado")
            
            return {
                "message": "Upload recebido (API de produÃ§Ã£o)",
                "filename": file.filename,
                "size": file.size,
                "note": "Para processamento completo, use a API principal"
            }
        
        @app.post("/track")
        async def track_basic(file: UploadFile = File(...)):
            return {
                "message": "Endpoint bÃ¡sico - use a API principal para processamento completo",
                "filename": file.filename,
                "status": "received"
            }
        
        print(f"ğŸŒ API de produÃ§Ã£o: http://{host}:{port}")
        print(f"ğŸ“– DocumentaÃ§Ã£o: http://{host}:{port}/docs")
        print("ğŸ’¡ Pressione Ctrl+C para parar")
        
        uvicorn.run(app, host=host, port=port, log_level="info")
        
    except ImportError as e:
        print(f"âŒ DependÃªncias da API nÃ£o encontradas: {e}")
        print("ğŸ’¡ Instale: pip install fastapi uvicorn python-multipart")
    except Exception as e:
        print(f"âŒ Erro na API bÃ¡sica: {e}")

# ğŸš« ============================================================================
# TREINAMENTO DESATIVADO - IMPLEMENTAÃ‡ÃƒO FUTURA VIA CI/CD PIPELINE
# ============================================================================

def cmd_train_detect_disabled(config_path, variant="n"):
    """ğŸš« TREINAMENTO DE DETECÃ‡ÃƒO - DESATIVADO"""
    print("\n" + "="*70)
    print("ğŸš« FUNCIONALIDADE DE TREINAMENTO TEMPORARIAMENTE DESATIVADA")
    print("="*70)
    print("ğŸ“‹ Status: Desabilitado para versÃ£o de produÃ§Ã£o")
    print("ğŸ”„ ImplementaÃ§Ã£o: Planejada para pipeline CI/CD")
    print("ğŸ¯ Objetivo: AutomatizaÃ§Ã£o via containers e cloud")
    print("ğŸ“… PrevisÃ£o: PrÃ³xima versÃ£o (v3.0)")
    print("")
    print("ğŸ’¡ PRÃ“XIMOS PASSOS:")
    print("   1. ConfiguraÃ§Ã£o de ambiente de treinamento isolado")
    print("   2. Pipeline automatizado com MLflow")
    print("   3. Deployment automÃ¡tico de modelos")
    print("   4. Monitoramento de performance")
    print("   5. Versionamento automÃ¡tico de modelos")
    print("")
    print("ğŸ”§ Para ativar temporariamente:")
    print("   - Modifique a flag 'training_disabled' no config")
    print("   - Ou implemente pipeline CI/CD personalizado")
    print("="*70)
    
    return {
        "success": False, 
        "status": "disabled",
        "message": "Treinamento desativado - implementaÃ§Ã£o futura via CI/CD",
        "next_steps": [
            "Setup CI/CD pipeline",
            "Container training environment",
            "MLflow integration",
            "Automated model deployment"
        ]
    }

def cmd_train_classify_disabled(config_path, variant="n", data_path=""):
    """ğŸš« TREINAMENTO DE CLASSIFICAÃ‡ÃƒO - DESATIVADO"""
    print("\n" + "="*70)
    print("ğŸš« FUNCIONALIDADE DE TREINAMENTO TEMPORARIAMENTE DESATIVADA")
    print("="*70)
    print("ğŸ“‹ Status: Desabilitado para versÃ£o de produÃ§Ã£o")
    print("ğŸ”„ ImplementaÃ§Ã£o: Planejada para pipeline CI/CD")
    print("ğŸ¯ Objetivo: AutomatizaÃ§Ã£o via containers e cloud")
    print("ğŸ“… PrevisÃ£o: PrÃ³xima versÃ£o (v3.0)")
    print("")
    print("ğŸ’¡ BENEFÃCIOS DO PIPELINE CI/CD:")
    print("   âœ… Treinamento automatizado em cloud")
    print("   âœ… Versionamento automÃ¡tico de modelos")
    print("   âœ… Testes automatizados de qualidade")
    print("   âœ… Deploy sem interrupÃ§Ã£o de serviÃ§o")
    print("   âœ… Rollback automÃ¡tico se performance cair")
    print("   âœ… Monitoramento contÃ­nuo de drift")
    print("")
    print("ğŸ—ï¸ ARQUITETURA PLANEJADA:")
    print("   ğŸ³ Docker containers para treinamento")
    print("   â˜ï¸ Cloud GPU instances sob demanda")
    print("   ğŸ“Š MLflow para tracking de experimentos")
    print("   ğŸš€ Kubernetes para orquestraÃ§Ã£o")
    print("   ğŸ“ˆ Prometheus para monitoramento")
    print("="*70)
    
    return {
        "success": False,
        "status": "disabled", 
        "message": "Treinamento desativado - implementaÃ§Ã£o futura via CI/CD",
        "planned_features": [
            "Automated cloud training",
            "Model versioning",
            "Quality gates",
            "Zero-downtime deployment",
            "Performance monitoring"
        ]
    }

def prompt_choice(title, options):
    """Menu de escolha otimizado"""
    print(f"\n{title}")
    for i, opt in enumerate(options, 1):
        print(f" {i}) {opt}")
    
    while True:
        try:
            choice = input("Escolha [nÃºmero]: ").strip()
            if choice.isdigit() and 1 <= int(choice) <= len(options):
                return int(choice) - 1
        except (KeyboardInterrupt, EOFError):
            print("\nğŸ‘‹ Saindo...")
            sys.exit(0)
        except:
            pass
        print("âŒ InvÃ¡lido. Tente novamente.")

def prompt_text(msg, default=None):
    """Prompt de texto otimizado"""
    prompt = f"{msg}"
    if default:
        prompt += f" [{default}]"
    prompt += ": "
    
    try:
        result = input(prompt).strip()
        return result if result else (default or "")
    except (KeyboardInterrupt, EOFError):
        print("\nğŸ‘‹ Saindo...")
        sys.exit(0)

def show_system_info():
    """Mostra informaÃ§Ãµes completas do sistema"""
    print("\nğŸ’» === INFORMAÃ‡Ã•ES DO SISTEMA - MODO PRODUÃ‡ÃƒO ===")
    
    # Python
    print(f"ğŸ Python: {sys.version.split()[0]} ({sys.platform})")
    print(f"ğŸ“‚ DiretÃ³rio atual: {Path.cwd()}")
    
    # PyTorch
    try:
        import torch
        print(f"ğŸ”¥ PyTorch: {torch.__version__}")
        print(f"ğŸ® CUDA disponÃ­vel: {'Sim' if torch.cuda.is_available() else 'NÃ£o'}")
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"ğŸ® GPU: {gpu_name} ({gpu_memory:.1f}GB)")
    except ImportError:
        print("ğŸ”¥ PyTorch: âŒ NÃ£o instalado")
    
    # Ultralytics
    try:
        from ultralytics import YOLO
        import ultralytics
        print(f"ğŸ¤– Ultralytics: {ultralytics.__version__}")
    except ImportError:
        print("ğŸ¤– Ultralytics: âŒ NÃ£o instalado")
    
    # OpenCV
    try:
        import cv2
        print(f"ğŸ“· OpenCV: {cv2.__version__}")
    except ImportError:
        print("ğŸ“· OpenCV: âŒ NÃ£o instalado")
    
    # FastAPI
    try:
        import fastapi
        print(f"ğŸš€ FastAPI: {fastapi.__version__}")
    except ImportError:
        print("ğŸš€ FastAPI: âŒ NÃ£o instalado")
    
    # Sistema
    try:
        import psutil
        cpu_count = psutil.cpu_count()
        cpu_freq = psutil.cpu_freq()
        ram = psutil.virtual_memory()
        print(f"ğŸ’» CPU: {cpu_count} cores @ {cpu_freq.current:.0f}MHz")
        print(f"ğŸ’¾ RAM: {ram.total/(1024**3):.1f}GB total")
    except ImportError:
        print("ğŸ’» System info: psutil nÃ£o disponÃ­vel")
    
    # Config e arquivos
    config_path = find_config_file()
    print(f"âš™ï¸ Config: {'âœ…' if Path(config_path).exists() else 'âŒ'} {config_path}")
    
    api_files = ["src/api/main_api.py", "main_api.py", "src/main_api.py"]
    api_found = any(Path(f).exists() for f in api_files)
    print(f"ğŸš€ API completa: {'âœ…' if api_found else 'âŒ'}")
    
    track_file = Path("src/tracking/track.py")
    print(f"ğŸ¯ Track module: {'âœ…' if track_file.exists() else 'âŒ'}")
    
    # Status de produÃ§Ã£o
    print(f"\nğŸ­ === STATUS DE PRODUÃ‡ÃƒO ===")
    print(f"ğŸ”§ Modo: ProduÃ§Ã£o (Production Mode)")
    print(f"ğŸš« Treinamento: Desativado")
    print(f"ğŸ“ˆ Tracking: Ativo")
    print(f"ğŸŒ API: Ativa")
    print(f"ğŸ”„ CI/CD: Planejado para v3.0")

def interactive_main():
    """Interface interativa - MODO PRODUÃ‡ÃƒO"""
    print("\nğŸš— === Vox - MODO PRODUÃ‡ÃƒO ===")
    print("ğŸ­ VersÃ£o de produÃ§Ã£o com foco em inferÃªncia")
    print("ğŸš« Treinamento desativado - Pipeline CI/CD em desenvolvimento")
    
    config_path = find_config_file()
    
    while True:
        choice = prompt_choice(
            "ğŸ¯ O que deseja fazer?",
            [
                "ğŸ¥ Processar vÃ­deo (Tracking + ClassificaÃ§Ã£o)",
                "ğŸš€ Iniciar API (Integrada com main_api.py)",
                "ğŸš« Treinar modelo de detecÃ§Ã£o (DESATIVADO)",
                "ğŸš« Treinar modelo de classificaÃ§Ã£o (DESATIVADO)",
                "â„¹ï¸ InformaÃ§Ãµes do sistema",
                "ğŸ§ª Testar dependÃªncias",
                "âŒ Sair"
            ]
        )
        
        if choice == 0:  # Tracking
            print("\nğŸ“¹ === PROCESSAMENTO DE VÃDEO ===")
            
            video = prompt_text("Caminho do vÃ­deo", "input.mp4")
            det_weights = prompt_text("Detector (.pt)", "runs/yolo11n_detection_detect3/weights/best.pt")
            cls_weights = prompt_text("Classificador (.pt)", "runs/yolo11n_classification_colors_n3/weights/best.pt")
            output_dir = prompt_text("DiretÃ³rio de saÃ­da (opcional)", "")
            
            print(f"\nğŸ”„ Processando em modo produÃ§Ã£o...")
            result = cmd_track_direct(video, det_weights, cls_weights, output_dir)
            
            if result:
                input("\nâœ… Processamento concluÃ­do! Pressione Enter para continuar...")
            else:
                input("\nâŒ Erro no processamento. Pressione Enter para continuar...")
        
        elif choice == 1:  # API Integrada
            print("\nğŸš€ === API MODO PRODUÃ‡ÃƒO ===")
            
            host = prompt_text("Host", "127.0.0.1")
            port_str = prompt_text("Porta", "8000")
            reload_str = prompt_text("Auto-reload? (y/N)", "n")
            
            try:
                port = int(port_str)
                reload = reload_str.lower().startswith('y')
                
                print(f"\nğŸŒ Iniciando API de produÃ§Ã£o em http://{host}:{port}")
                cmd_api_integrated(host, port, reload)
            except ValueError:
                print("âŒ Porta invÃ¡lida")
                input("Pressione Enter para continuar...")
        
        elif choice == 2:  # Treinar detecÃ§Ã£o - DESATIVADO
            print("\nğŸš« === TREINAMENTO DE DETECÃ‡ÃƒO - DESATIVADO ===")
            variant = prompt_text("Variante do modelo (n/s/m/l/x)", "n")
            cmd_train_detect_disabled(config_path, variant)
            input("\nPressione Enter para continuar...")
        
        elif choice == 3:  # Treinar classificaÃ§Ã£o - DESATIVADO
            print("\nğŸš« === TREINAMENTO DE CLASSIFICAÃ‡ÃƒO - DESATIVADO ===")
            variant = prompt_text("Variante do modelo (n/s/m/l/x)", "s")
            cmd_train_classify_disabled(config_path, variant)
            input("\nPressione Enter para continuar...")
        
        elif choice == 4:  # Info do sistema
            show_system_info()
            input("\nPressione Enter para continuar...")
        
        elif choice == 5:  # Teste de dependÃªncias
            print("\nğŸ§ª === TESTANDO DEPENDÃŠNCIAS - MODO PRODUÃ‡ÃƒO ===")
            
            # DependÃªncias essenciais para produÃ§Ã£o
            production_dependencies = [
                ("torch", "PyTorch"),
                ("ultralytics", "Ultralytics YOLO"),
                ("cv2", "OpenCV"),
                ("PIL", "Pillow"),
                ("fastapi", "FastAPI"),
                ("uvicorn", "Uvicorn"),
                ("psutil", "psutil")
            ]
            
            all_ok = True
            for module_name, display_name in production_dependencies:
                try:
                    __import__(module_name)
                    print(f"âœ… {display_name}: OK")
                except ImportError:
                    print(f"âŒ {display_name}: FALTANDO")
                    all_ok = False
            
            print("\nğŸ” DependÃªncias opcionais para treinamento:")
            training_dependencies = [
                ("mlflow", "MLflow (CI/CD)"),
                ("wandb", "Weights & Biases (CI/CD)"),
                ("tensorboard", "TensorBoard (CI/CD)")
            ]
            
            for module_name, display_name in training_dependencies:
                try:
                    __import__(module_name)
                    print(f"ğŸ”„ {display_name}: DisponÃ­vel (nÃ£o usado em produÃ§Ã£o)")
                except ImportError:
                    print(f"ğŸš« {display_name}: NÃ£o instalado (serÃ¡ usado em CI/CD)")
            
            if all_ok:
                print("\nğŸ‰ Todas as dependÃªncias de produÃ§Ã£o estÃ£o instaladas!")
                print("ğŸ­ Sistema pronto para uso em produÃ§Ã£o!")
            else:
                print("\nâš ï¸ Instale dependÃªncias de produÃ§Ã£o:")
                print("pip install torch ultralytics opencv-python pillow fastapi uvicorn psutil python-multipart")
            
            input("\nPressione Enter para continuar...")
        
        else:  # Sair
            print("\nğŸ‘‹ Saindo do modo produÃ§Ã£o... AtÃ© mais!")
            break

def build_parser():
    """Parser com comandos de produÃ§Ã£o"""
    parser = argparse.ArgumentParser(
        description="Vox - Sistema de ProduÃ§Ã£o v2.0",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ­ MODO PRODUÃ‡ÃƒO - Exemplos de uso:
  python main_uni.py                                    # Modo interativo
  python main_uni.py track --video input.mp4 --det-weights detector.pt --cls-weights classifier.pt
  python main_uni.py api --host 0.0.0.0 --port 8000
  
ğŸš« TREINAMENTO DESATIVADO:
  - Funcionalidades de treinamento estÃ£o desabilitadas
  - ImplementaÃ§Ã£o planejada via pipeline CI/CD
  - Para desenvolvimento, use versÃ£o separada

ğŸ’¡ Para ativar treinamento:
  - Configure pipeline CI/CD personalizado
  - Ou modifique flags no config.json
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", required=False)
    
    # Track command (ATIVO)
    track_parser = subparsers.add_parser("track", help="Processar vÃ­deo (PRODUÃ‡ÃƒO)")
    track_parser.add_argument("--video", required=True, help="Caminho do vÃ­deo")
    track_parser.add_argument("--det-weights", required=True, help="Pesos do detector (.pt)")
    track_parser.add_argument("--cls-weights", required=True, help="Pesos do classificador (.pt)")
    track_parser.add_argument("--output", default="", help="DiretÃ³rio de saÃ­da")
    
    # API command (ATIVO)
    api_parser = subparsers.add_parser("api", help="Iniciar API (PRODUÃ‡ÃƒO)")
    api_parser.add_argument("--host", default="127.0.0.1", help="Host da API")
    api_parser.add_argument("--port", type=int, default=8000, help="Porta da API")
    api_parser.add_argument("--reload", action="store_true", help="Auto-reload")
    
    # Train commands (DESATIVADOS)
    train_detect_parser = subparsers.add_parser("train-detect", help="Treinar detecÃ§Ã£o (DESATIVADO)")
    train_detect_parser.add_argument("--variant", default="n", choices=['n', 's', 'm', 'l', 'x'], help="Variante do modelo")
    
    train_classify_parser = subparsers.add_parser("train-classify", help="Treinar classificaÃ§Ã£o (DESATIVADO)")
    train_classify_parser.add_argument("--variant", default="s", choices=['n', 's', 'm', 'l', 'x'], help="Variante do modelo")
    
    # Other commands
    subparsers.add_parser("interactive", help="Modo interativo (PRODUÃ‡ÃƒO)")
    subparsers.add_parser("info", help="InformaÃ§Ãµes do sistema")
    
    return parser

def main():
    """FunÃ§Ã£o principal - MODO PRODUÃ‡ÃƒO"""
    print("ğŸ­ Vox - MODO PRODUÃ‡ÃƒO")
    print("ğŸš« Treinamento desativado | ğŸ¯ Foco em inferÃªncia")
    
    parser = build_parser()
    args = parser.parse_args()
    
    # Se nenhum comando, inicia interativo
    if args.command is None:
        print("â„¹ï¸ Iniciando modo interativo de produÃ§Ã£o...")
        interactive_main()
        return
    
    # Comandos CLI
    config_path = find_config_file()
    
    if args.command == "track":
        result = cmd_track_direct(args.video, args.det_weights, args.cls_weights, args.output)
        return result
    
    elif args.command == "api":
        cmd_api_integrated(args.host, args.port, args.reload)
        return
    
    elif args.command == "train-detect":
        result = cmd_train_detect_disabled(config_path, args.variant)
        return result
    
    elif args.command == "train-classify":
        result = cmd_train_classify_disabled(config_path, args.variant)
        return result
    
    elif args.command == "interactive":
        interactive_main()
        return
    
    elif args.command == "info":
        show_system_info()
        return
    
    else:
        print(f"âŒ Comando desconhecido: {args.command}")
        parser.print_help()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Interrompido pelo usuÃ¡rio!")
    except Exception as e:
        print(f"\nâŒ Erro inesperado: {e}")
        import traceback
        traceback.print_exc()