{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff02d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> VeRi\n",
      "-> VCoR\n",
      "OK VeRi cars: 43939 | VCoR imgs: 7746 | Classes: ['amarelo', 'azul', 'branco', 'cinza_prata', 'dourado', 'laranja', 'marrom', 'preto', 'verde', 'vermelho'] | Final: C:\\Users\\riana\\OneDrive\\Desktop\\Vox MVP\\data\\PROCESSED\\COLOR_FINAL_YOLO\n"
     ]
    }
   ],
   "source": [
    "# Pré-processamento com verificação e retomada (10 cores VeRi, sem limite)\n",
    "# - Corrige XML gb2312 (gb18030/gbk -> UTF-8)\n",
    "# - Filtra somente \"carros\" (VeRi) e somente as 10 cores do VeRi\n",
    "# - Verifica pastas existentes, evita duplicatas e resume sem sobrescrever\n",
    "# - Gera manifest/resumo por split/classe\n",
    "# Saída: COLOR_FINAL_YOLO no formato Ultralytics (pastas por classe + data.yaml)\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import shutil, json, re\n",
    "\n",
    "# ---------- Constantes ----------\n",
    "VERI_COLOR_ID = {1:\"yellow\",2:\"orange\",3:\"green\",4:\"gray\",5:\"red\",6:\"blue\",7:\"white\",8:\"golden\",9:\"brown\",10:\"black\"}  # cores VeRi [1]\n",
    "CAR_TYPES = {1,2,4,5,9}  # sedan, suv, hatchback, mpv, estate [1]\n",
    "COLOR_MAP = {\n",
    "    \"yellow\":\"amarelo\",\"orange\":\"laranja\",\"green\":\"verde\",\"gray\":\"cinza_prata\",\"red\":\"vermelho\",\n",
    "    \"blue\":\"azul\",\"white\":\"branco\",\"golden\":\"dourado\",\"brown\":\"marrom\",\"black\":\"preto\",\n",
    "    \"grey\":\"cinza_prata\",\"silver\":\"cinza_prata\",\"beige\":\"bege\",\"gold\":\"dourado\",\"purple\":\"roxo\",\"pink\":\"rosa\",\"tan\":\"bege\"\n",
    "}  # normalização p/ pastas por classe Ultralytics [1]\n",
    "ALLOWED_10 = {\"amarelo\",\"laranja\",\"verde\",\"cinza_prata\",\"vermelho\",\"azul\",\"branco\",\"dourado\",\"marrom\",\"preto\"}  # paleta VeRi [1]\n",
    "\n",
    "# ---------- Utilidades de arquivo ----------\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)  # cria diretório de forma idempotente [2]\n",
    "\n",
    "def list_files(dirp: Path):\n",
    "    return {f.name for f in dirp.glob(\"*.*\")} if dirp.exists() else set()  # inventário rápido por pasta [2]\n",
    "\n",
    "def safe_copy(src: Path, dst_dir: Path, prefix: str, allow_collision_suffix=True):\n",
    "    \"\"\"Copia evitando sobrescrever; se existir o mesmo nome, adiciona sufixo incremental.\"\"\"\n",
    "    if not src.exists():\n",
    "        return None\n",
    "    ensure_dir(dst_dir)\n",
    "    base = f\"{prefix}_{src.name}\"\n",
    "    dst = dst_dir / base\n",
    "    if not dst.exists():\n",
    "        shutil.copy2(src, dst)\n",
    "        return dst\n",
    "    if not allow_collision_suffix:\n",
    "        return None\n",
    "    # cria sufixos _1, _2, ...\n",
    "    stem, suf = dst.stem, dst.suffix\n",
    "    i = 1\n",
    "    while True:\n",
    "        cand = dst_dir / f\"{stem}_{i}{suf}\"\n",
    "        if not cand.exists():\n",
    "            shutil.copy2(src, cand)\n",
    "            return cand\n",
    "        i += 1  # caminho existe; tenta próximo [2]\n",
    "\n",
    "def load_xml_root_utf8(xml_path: Path):\n",
    "    \"\"\"Lê XML VeRi declarados como gb2312/GBK/GB18030 e normaliza para UTF-8 (correção ElementTree).\"\"\"\n",
    "    data = xml_path.read_bytes()\n",
    "    text = None\n",
    "    for enc in (\"gb18030\",\"gbk\",\"utf-8\"):\n",
    "        try:\n",
    "            text = data.decode(enc); break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    if text is None:\n",
    "        text = data.decode(\"latin-1\", errors=\"ignore\")\n",
    "    if text.lstrip().startswith(\"<?xml\"):\n",
    "        text = re.sub(r'encoding=[\\'\\\"].*?[\\'\\\"]', 'encoding=\"utf-8\"', text, count=1)\n",
    "    else:\n",
    "        text = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n' + text\n",
    "    return ET.fromstring(text.encode(\"utf-8\"))  # evita erro do Expat com multibyte [2]\n",
    "\n",
    "def summarize_tree(root_dir: Path):\n",
    "    \"\"\"Resumo por split/classe: contagem de arquivos, útil para auditoria e retomada.\"\"\"\n",
    "    summary = defaultdict(lambda: defaultdict(int))\n",
    "    if not root_dir.exists():\n",
    "        return summary\n",
    "    for split in root_dir.iterdir():\n",
    "        if not split.is_dir():\n",
    "            continue\n",
    "        for cls in split.iterdir():\n",
    "            if cls.is_dir():\n",
    "                summary[split.name][cls.name] = sum(1 for _ in cls.glob(\"*.*\"))\n",
    "    return summary  # ajuda a validar formato Ultralytics (pastas por classe) [1]\n",
    "\n",
    "def save_json(obj, path: Path):\n",
    "    ensure_dir(path.parent)\n",
    "    path.write_text(json.dumps(obj, indent=2, ensure_ascii=False), encoding=\"utf-8\")  # persistência idempotente [2]\n",
    "\n",
    "# ---------- Processamentos ----------\n",
    "def process_veri(veri_root: Path, out_root: Path) -> dict:\n",
    "    \"\"\"Extrai APENAS carros do VeRi e organiza por cor normalizada nas 10 cores.\"\"\"\n",
    "    stats = {\"total\":0,\"cars\":0,\"colors\":defaultdict(int)}\n",
    "    veri_out = out_root / \"veri_processed\"\n",
    "    cfgs = [(\"train\",\"train_label.xml\",\"image_train\"), (\"test\",\"test_label.xml\",\"image_test\")]\n",
    "    if (veri_root/\"image_query\").exists():\n",
    "        cfgs.append((\"query\",\"test_label.xml\",\"image_query\"))\n",
    "    for split, xmlf, imdir in cfgs:\n",
    "        x, d = veri_root/xmlf, veri_root/imdir\n",
    "        if not x.exists() or not d.exists():\n",
    "            continue\n",
    "        root = load_xml_root_utf8(x)\n",
    "        for it in root.iter(\"Item\"):\n",
    "            stats[\"total\"] += 1\n",
    "            cid, tid = int(it.attrib[\"colorID\"]), int(it.attrib[\"typeID\"])\n",
    "            if tid not in CAR_TYPES:\n",
    "                continue\n",
    "            color_pt = COLOR_MAP.get(VERI_COLOR_ID.get(cid,\"unknown\"), \"outros\")\n",
    "            if color_pt not in ALLOWED_10:\n",
    "                continue\n",
    "            stats[\"cars\"] += 1\n",
    "            stats[\"colors\"][color_pt] += 1\n",
    "            src = d / it.attrib[\"imageName\"]\n",
    "            dst_dir = veri_out / split / color_pt\n",
    "            # cópia segura com retomada\n",
    "            safe_copy(src, dst_dir, \"veri\")  # não sobrescreve; cria sufixo se colidir [2]\n",
    "    return stats  # atende formato de pastas por classe adotado no Ultralytics classify [1]\n",
    "\n",
    "def process_vcor(vcor_root: Path, out_root: Path) -> dict:\n",
    "    \"\"\"Padroniza nomes de cor do VCoR e copia apenas 10 cores para pastas por classe em train/val/test.\"\"\"\n",
    "    stats = {\"total\":0,\"colors\":defaultdict(int)}\n",
    "    vcor_out = out_root / \"vcor_processed\"\n",
    "    for split in (\"train\",\"val\",\"test\"):\n",
    "        sdir = vcor_root/split\n",
    "        if not sdir.exists():\n",
    "            continue\n",
    "        for cdir in sdir.iterdir():\n",
    "            if not cdir.is_dir():\n",
    "                continue\n",
    "            cor = COLOR_MAP.get(cdir.name.lower(), cdir.name.lower())\n",
    "            if cor not in ALLOWED_10:\n",
    "                continue\n",
    "            dst = vcor_out/split/cor\n",
    "            # inventário existente para retomar\n",
    "            existing = list_files(dst)\n",
    "            for img in cdir.glob(\"*.*\"):\n",
    "                if img.suffix.lower() in (\".jpg\",\".jpeg\",\".png\",\".bmp\"):\n",
    "                    stats[\"total\"] += 1\n",
    "                    stats[\"colors\"][cor] += 1\n",
    "                    # se já existir um nome igual, safe_copy criará sufixo\n",
    "                    safe_copy(img, dst, \"vcor\")  # idempotente em reexecução [2]\n",
    "    return stats\n",
    "\n",
    "def merge_10(veri_proc: Path, vcor_proc: Path, out_final: Path) -> dict:\n",
    "    \"\"\"Une VeRi (train/test/query->train) + VCoR (train/val/test) nas 10 cores, sem limite e sem sobrescrever.\"\"\"\n",
    "    final_stats = defaultdict(lambda: defaultdict(int))\n",
    "    pools = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Mapear VeRi tudo -> train; VCoR: manter splits\n",
    "    for base, split_map, tag in [\n",
    "        (veri_proc, {\"train\":\"train\",\"test\":\"train\",\"query\":\"train\"}, \"veri\"),\n",
    "        (vcor_proc, {\"train\":\"train\",\"val\":\"val\",\"test\":\"test\"}, \"vcor\")\n",
    "    ]:\n",
    "        if not base.exists():\n",
    "            continue\n",
    "        for sdir in base.iterdir():\n",
    "            if not sdir.is_dir() or sdir.name not in split_map:\n",
    "                continue\n",
    "            dst_split = split_map[sdir.name]\n",
    "            for cdir in sdir.iterdir():\n",
    "                if cdir.is_dir() and cdir.name in ALLOWED_10:\n",
    "                    pools[dst_split][cdir.name].extend(cdir.glob(\"*.*\"))\n",
    "\n",
    "    # Copiar todos (sem limite), com verificação de existentes\n",
    "    for split, cmap in pools.items():\n",
    "        for color, imgs in cmap.items():\n",
    "            dst = out_final/split/color\n",
    "            ensure_dir(dst)\n",
    "            existing = list_files(dst)\n",
    "            idx = len(existing)  # continua numeração\n",
    "            for src in imgs:\n",
    "                name = f\"{color}_{idx:06d}{src.suffix}\"\n",
    "                idx += 1\n",
    "                dst_path = dst / name\n",
    "                if dst_path.name in existing:\n",
    "                    continue  # já existe\n",
    "                shutil.copy2(src, dst_path)\n",
    "                final_stats[split][color] += 1\n",
    "\n",
    "    names = sorted(ALLOWED_10)\n",
    "    ensure_dir(out_final)\n",
    "    (out_final/\"data.yaml\").write_text(\n",
    "        f\"path: {out_final.resolve()}\\ntrain: train\\nval: val\\n\"\n",
    "        + (\"test: test\\n\" if (out_final/'test').exists() else \"\")\n",
    "        + f\"\\nnc: {len(names)}\\nnames: {names}\\n\", encoding=\"utf-8\"\n",
    "    )  # estrutura esperada pelo yolo classify [1]\n",
    "    save_json(final_stats, out_final/\"dataset_stats.json\")\n",
    "    return {\"classes\": names, \"final_stats\": final_stats}\n",
    "\n",
    "def run(veri_path: str, vcor_path: str, out_path: str):\n",
    "    veri_root, vcor_root, out_root = Path(veri_path), Path(vcor_path), Path(out_path)\n",
    "    # Verificações iniciais de existência\n",
    "    assert veri_root.exists(), f\"VeRi não encontrado: {veri_root}\"  # checagem básica com Path.exists [2]\n",
    "    assert vcor_root.exists(), f\"VCoR não encontrado: {vcor_root}\"  # idem [2]\n",
    "    ensure_dir(out_root)\n",
    "\n",
    "    print(\"-> VeRi\"); veri_stats = process_veri(veri_root, out_root)\n",
    "    print(\"-> VCoR\"); vcor_stats = process_vcor(vcor_root, out_root)\n",
    "\n",
    "    # Resumos intermediários (inventário)\n",
    "    save_json(summarize_tree(out_root/\"veri_processed\"), out_root/\"veri_processed_manifest.json\")\n",
    "    save_json(summarize_tree(out_root/\"vcor_processed\"), out_root/\"vcor_processed_manifest.json\")\n",
    "\n",
    "    # Merge final\n",
    "    final_dir = out_root/\"COLOR_FINAL_YOLO\"\n",
    "    info = merge_10(out_root/\"veri_processed\", out_root/\"vcor_processed\", final_dir)\n",
    "    save_json(summarize_tree(final_dir), final_dir/\"final_manifest.json\")\n",
    "\n",
    "    print(f\"OK VeRi cars: {veri_stats['cars']} | VCoR imgs: {vcor_stats['total']} | Classes: {info['classes']} | Final: {final_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run(\n",
    "        r\"C:\\Users\\riana\\OneDrive\\Desktop\\Vox MVP\\data\\VeRi\",\n",
    "        r\"C:\\Users\\riana\\OneDrive\\Desktop\\Vox MVP\\data\\VCor\",\n",
    "        r\"C:\\Users\\riana\\OneDrive\\Desktop\\Vox MVP\\data\\PROCESSED\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "547fc94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta de destino C:\\Users\\riana\\OneDrive\\Desktop\\Vox MVP\\data\\PROCESSED\\UA_DETRAC_CAR já existe. Limpando...\n",
      "Usando ID de 'car' pré-definido: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando split 'train': 100%|██████████| 83791/83791 [12:23<00:00, 112.72it/s] \n",
      "Processando split 'val': 100%|██████████| 56340/56340 [08:28<00:00, 110.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Resumo do Processamento:\n",
      "{\n",
      "  \"car_id_used\": 3,\n",
      "  \"source_class_frequencies\": {},\n",
      "  \"splits\": {\n",
      "    \"train\": {\n",
      "      \"images\": 83791,\n",
      "      \"labels_in\": 82085,\n",
      "      \"boxes_total\": 598281,\n",
      "      \"boxes_kept\": 33651,\n",
      "      \"empties\": 58161\n",
      "    },\n",
      "    \"val\": {\n",
      "      \"images\": 56340,\n",
      "      \"labels_in\": 56167,\n",
      "      \"boxes_total\": 675774,\n",
      "      \"boxes_kept\": 71785,\n",
      "      \"empties\": 25418\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "✅ Dataset pronto em: C:\\Users\\riana\\OneDrive\\Desktop\\Vox MVP\\data\\PROCESSED\\UA_DETRAC_CAR\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# UA-DETRAC -> YOLO (1 classe: car) para PROCESSED/UA_DETRAC_CAR\n",
    "# - Foco total na classe \"car\" (ID original informável via CAR_ID_OVERRIDE)\n",
    "# - Mantém apenas \"car\" e remapeia para classe 0 (zero-based, contínua)\n",
    "# - Verifica/normaliza xywh para [0,1] e descarta boxes degenerados\n",
    "# - Copia imagens por hardlink (quando possível) + fallback copy2\n",
    "# - Aceita múltiplas extensões (.jpg, .jpeg, .png, .bmp)\n",
    "# - Gera detrac_car.yaml no padrão Ultralytics e manifest JSON\n",
    "# - Barras de progresso com tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CONFIG: ajuste as pastas de origem/destino\n",
    "SRC_ROOT = Path(r\"C:\\Users\\riana\\OneDrive\\Desktop\\Vox MVP\\data\\UA-DETRAC\\DETRAC_Upload\")\n",
    "DST_ROOT = Path(r\"C:\\Users\\riana\\OneDrive\\Desktop\\Vox MVP\\data\\PROCESSED\\UA_DETRAC_CAR\")\n",
    "\n",
    "SRC_IM = {\"train\": SRC_ROOT/\"images\"/\"train\", \"val\": SRC_ROOT/\"images\"/\"val\"}\n",
    "SRC_LB = {\"train\": SRC_ROOT/\"labels\"/\"train\", \"val\": SRC_ROOT/\"labels\"/\"val\"}\n",
    "\n",
    "DST_IM = {\"train\": DST_ROOT/\"images\"/\"train\", \"val\": DST_ROOT/\"images\"/\"val\"}\n",
    "DST_LB = {\"train\": DST_ROOT/\"labels\"/\"train\", \"val\": DST_ROOT/\"labels\"/\"val\"}\n",
    "\n",
    "# Se souber o ID de 'car' nos rótulos originais, defina aqui (ex.: 3);\n",
    "# caso contrário, deixe None para detecção automática pela classe mais frequente.\n",
    "CAR_ID_OVERRIDE = 3  # defina 3 se sua taxonomia original for aquela lista com \"car\"=3\n",
    "\n",
    "EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\")  # suportadas pelo loader do YOLO\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def read_label_file(p: Path):\n",
    "    if not p.exists():\n",
    "        return []\n",
    "    rows = []\n",
    "    for line in p.read_text(encoding=\"utf-8\").splitlines():\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 5:\n",
    "            try:\n",
    "                c = int(parts[0]); x,y,w,h = map(float, parts[1:])\n",
    "                rows.append([c,x,y,w,h])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return rows\n",
    "\n",
    "def is_normalized_xywh(rows):\n",
    "    for _, x, y, w, h in rows:\n",
    "        if max(x, y, w, h) > 1.5:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def clamp01(v): \n",
    "    return max(0.0, min(1.0, v))\n",
    "\n",
    "def xyxy_to_xywh_norm(x1, y1, x2, y2, W, H):\n",
    "    xc = ((x1 + x2) / 2.0) / W\n",
    "    yc = ((y1 + y2) / 2.0) / H\n",
    "    ww = (abs(x2 - x1)) / W\n",
    "    hh = (abs(y2 - y1)) / H\n",
    "    return clamp01(xc), clamp01(yc), clamp01(ww), clamp01(hh)\n",
    "\n",
    "# Normalização com W,H já conhecidos\n",
    "def normalize_rows(rows, W, H):\n",
    "    if not rows:\n",
    "        return []\n",
    "    if is_normalized_xywh(rows):\n",
    "        return [[c, clamp01(x), clamp01(y), clamp01(w), clamp01(h)] for c,x,y,w,h in rows]\n",
    "    nr = []\n",
    "    for c, a, b, c3, d in rows:\n",
    "        if c3 > 1.5 and d > 1.5:  # assume xywh em pixels\n",
    "            xc = a / W; yc = b / H; ww = c3 / W; hh = d / H\n",
    "        else:  # assume xyxy em pixels\n",
    "            x1, y1, x2, y2 = a, b, c3, d\n",
    "            xc, yc, ww, hh = xyxy_to_xywh_norm(x1, y1, x2, y2, W, H)\n",
    "        nr.append([c, clamp01(xc), clamp01(yc), clamp01(ww), clamp01(hh)])\n",
    "    return nr\n",
    "\n",
    "def discover_car_id():\n",
    "    cls_freq = Counter()\n",
    "    all_labels = list(SRC_LB[\"train\"].glob(\"*.txt\")) + list(SRC_LB[\"val\"].glob(\"*.txt\"))\n",
    "    for txt in tqdm(all_labels, desc=\"Descobrindo ID de 'car'\"):\n",
    "        for r in read_label_file(txt):\n",
    "            cls_freq[r[0]] += 1\n",
    "    if not cls_freq:\n",
    "        return 0, {}\n",
    "    car_id = cls_freq.most_common(1)[0][0]  # 'car' tende a ser o mais frequente\n",
    "    return car_id, dict(cls_freq)\n",
    "\n",
    "def copy_image(src_img: Path, dst_img: Path):\n",
    "    ensure_dir(dst_img.parent)\n",
    "    if dst_img.exists():\n",
    "        return\n",
    "    try:\n",
    "        os.link(src_img, dst_img)  # hardlink (mesmo volume)\n",
    "    except Exception:\n",
    "        shutil.copy2(src_img, dst_img)  # fallback\n",
    "\n",
    "def write_yaml(dst_root: Path):\n",
    "    yaml = (\n",
    "        f\"path: {dst_root.resolve()}\\n\"\n",
    "        f\"train: images/train\\n\"\n",
    "        f\"val: images/val\\n\"\n",
    "        f\"\\nnc: 1\\n\"\n",
    "        f\"names: [car]\\n\"\n",
    "    )\n",
    "    (dst_root/\"detrac_car.yaml\").write_text(yaml, encoding=\"utf-8\")\n",
    "\n",
    "def collect_images(images_dir: Path):\n",
    "    paths = []\n",
    "    for ext in EXTS:\n",
    "        paths.extend(images_dir.rglob(f\"*{ext}\"))\n",
    "    return sorted(paths)\n",
    "\n",
    "def process_split(split, car_id: int, summary: dict):\n",
    "    ensure_dir(DST_IM[split]); ensure_dir(DST_LB[split])\n",
    "    stats = {\"images\": 0, \"labels_in\": 0, \"boxes_total\": 0, \"boxes_kept\": 0, \"empties\": 0}\n",
    "    img_paths = collect_images(SRC_IM[split])\n",
    "\n",
    "    # Cache de dimensões por pasta absoluta para evitar colisões\n",
    "    dim_cache = {}\n",
    "\n",
    "    for src_img in tqdm(img_paths, desc=f\"Processando split '{split}'\"):\n",
    "        stem = src_img.stem\n",
    "        src_lbl = SRC_LB[split] / f\"{stem}.txt\"\n",
    "        \n",
    "        rows = read_label_file(src_lbl)\n",
    "        if rows:\n",
    "            stats[\"labels_in\"] += 1\n",
    "        stats[\"boxes_total\"] += len(rows)\n",
    "\n",
    "        # Copia imagem (YOLO ignora imagens sem .txt)\n",
    "        dst_img = DST_IM[split] / src_img.name\n",
    "        copy_image(src_img, dst_img)\n",
    "        stats[\"images\"] += 1\n",
    "\n",
    "        # Manter apenas car_id\n",
    "        car_rows = [r for r in rows if int(r[0]) == car_id]\n",
    "        if not car_rows:\n",
    "            # garante que não haja .txt antigo no destino\n",
    "            dst_lbl = DST_LB[split] / f\"{stem}.txt\"\n",
    "            if dst_lbl.exists():\n",
    "                dst_lbl.unlink()\n",
    "            stats[\"empties\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Dimensões por cache\n",
    "        seq_key = src_img.parent.as_posix()\n",
    "        if seq_key not in dim_cache:\n",
    "            img = cv2.imread(str(src_img))\n",
    "            H, W = img.shape[:2]\n",
    "            dim_cache[seq_key] = (W, H)\n",
    "        else:\n",
    "            W, H = dim_cache[seq_key]\n",
    "\n",
    "        # Normalizar\n",
    "        normalized = normalize_rows(car_rows, W, H)\n",
    "\n",
    "        # Descartar boxes degenerados (área muito pequena / fora de [0,1] antes do clamp final)\n",
    "        filtered = []\n",
    "        for _, x, y, w, h in normalized:\n",
    "            if 0.0 <= x <= 1.0 and 0.0 <= y <= 1.0 and 0.0 < w <= 1.0 and 0.0 < h <= 1.0 and (w * h) >= 1e-6:\n",
    "                filtered.append((x, y, w, h))\n",
    "\n",
    "        stats[\"boxes_kept\"] += len(filtered)\n",
    "\n",
    "        dst_lbl = DST_LB[split] / f\"{stem}.txt\"\n",
    "        if filtered:\n",
    "            with dst_lbl.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                for x, y, w, h in filtered:\n",
    "                    f.write(f\"0 {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "        else:\n",
    "            if dst_lbl.exists():\n",
    "                dst_lbl.unlink()\n",
    "            stats[\"empties\"] += 1\n",
    "\n",
    "    summary[\"splits\"][split] = stats\n",
    "\n",
    "def main():\n",
    "    # Limpar destino para exportação consistente\n",
    "    if DST_ROOT.exists():\n",
    "        print(f\"Pasta de destino {DST_ROOT} já existe. Limpando...\")\n",
    "        shutil.rmtree(DST_ROOT)\n",
    "    ensure_dir(DST_ROOT)\n",
    "\n",
    "    # Definir ou descobrir CAR_ID\n",
    "    if CAR_ID_OVERRIDE is not None:\n",
    "        car_id, freq = CAR_ID_OVERRIDE, {}\n",
    "        print(f\"Usando ID de 'car' pré-definido: {car_id}\")\n",
    "    else:\n",
    "        print(\"Buscando ID da classe mais comum ('car')...\")\n",
    "        car_id, freq = discover_car_id()\n",
    "\n",
    "    summary = {\"car_id_used\": car_id, \"source_class_frequencies\": freq, \"splits\": {}}\n",
    "\n",
    "    # Processar splits\n",
    "    for split in (\"train\", \"val\"):\n",
    "        process_split(split, car_id, summary)\n",
    "\n",
    "    # YAML e manifestos\n",
    "    write_yaml(DST_ROOT)\n",
    "    manifest_path = DST_ROOT / \"ua_detrac_preprocess_manifest.json\"\n",
    "    manifest_path.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Resumo do Processamento:\")\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    print(f\"\\n✅ Dataset pronto em: {DST_ROOT.resolve()}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80db9c",
   "metadata": {},
   "source": [
    "UA-DETRAC → Detecção + Tracking de veículos\n",
    "\n",
    "     ↓\n",
    "\n",
    "VCoR + VeRi → Treinamento classificador de cores  \n",
    "\n",
    "     ↓\n",
    "     \n",
    "Integração → Sistema completo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
